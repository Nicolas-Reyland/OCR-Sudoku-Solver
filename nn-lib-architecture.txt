// Architecture of the nn C-lib

Model :
  Model *
    - (ModelLayers) layers
    - (functions->losses) loss
    - (functions->optimizers) optimizer
    - ($void) save // save_architecture & save_weights_and_bias
    - ($void) saveArchitecture
    - ($void) saveWeightsAndBias
  ($void) loadModel
  Layers :
    ModelLayers *
      - (uint) num_layers
      - (Layer) input_layer
      - (uint) num_hidden
      - (malloc array) hidden_layers // en soit, le projet ne demande qu'un seul hiddenLayer
      - (Layer) output_layer
    Layer *
      - (bool) is_output
      - (Layer) next_layer
      - (utils->Struct->ShapeDescription) shape
      - (activation) activation
      - (linked-list-Node) nodes
    Node *
     - (double-array) weights //il pointe vers le layer d'avant
     - (double) bias


for (int = 0; i < hidden_layer.length; i++) {
  double sum = 0;
  for (int j = 0; j < input_layer.length: j++) {
    sum += hidden_layer.nodes[i].weights[j] * inputs[j];
  }
  hidden_layer.raw_value = sum + hidden_layer.bias[i];
  hidden_layer.value = hidden_layer->activation_fn(hidden_layer.raw_value);
  // value = sigmoid(raw_value)
}

Data :
  DataCollectionTuple *
    - (DataCollection) data1
    - (DataCollection) data2
  DataCollection *
    - (utils->Struct->ShapeDescription) shape
    - (void* - linked-list) data
    - ($DataCollectionTuple) splitTrainTest
    - ($void) loadData
  Data *
    - (DataCollection) train_X    // ~ 70 %
    - (DataCollection) train_Y    // ~ 70 %
    - (DataCollection) test_X     // ~ 30 %
    - (DataCollection) test_Y     // ~ 30 %

Session :
  Session *
    - (Data) data
    - (uint) num_epochs
    - (double) loss_threshold
    - (bool) stop_on_loss_threshold_reached
    - (bool) verbose
    - ($void) train
    - ($void) test

FunctionsDescriptor :       //functions that will be applied on model(or for stats)
  losses +       //taux d'erreurs
    CategoricalCrossentropy
    BinaryCrossentropy
    SparseCategoricalCrossentropy
  activations +
    sigmoid
    relu
    leaky_relu    //optional
    softmax
    tanh
  optimizers +
    RMSprop
    Adam

utils :
  Struct :
    ShapeDescription *
      - (uint) dims
      - (uint) x
      - (uint) y
      - (uint) z
  Functions :
    losses :
      ($double) CategoricalCrossentropy
      ($double) BinaryCrossentropy
      ($double) SparseCategoricalCrossentropy
    activations :
      ($double) sigmoid
      ($double) relu
      ($double) leaky_relu    //optional
      ($double) softmax
      tanh
    optimizers :
      ($void) RMSprop
      ($void) Adam



/* Norme d'ecriture */

Struct    : CamelCase (nn_[StructName])
Enum      : FULLCAPS (ENUMVARIABLE) && pascalCase for Name (enum.ENUMVARIABLE)
Function  : pascalCase (functionName)
Variable  : snake_case (variable_name)
Constant (define prepo) : FULLCAPS

/* Vocab */
* : Struct
+ : enum
$ : function



/* data file format */
input file content :
256 ...
0 0
1 0
0 1

output file content :
0
0
1
1

/* File Architecture */
Model :
  Model * model.h/c
  Layers :
    ModelLayers * model_layers.h/c
    Layer * layer.h/c

Data :
  DataCollectionTuple * data_collection_tuple.h/c
  DataCollection * data_collection.h/c
  Data * data.h/c

Session :
  Session * session.h/c

FunctionsDescriptor :
  functions_descriptors_enums.h/c

utils :
  Struct :
    ShapeDescription * shape_description.h/c
  Functions :
    losses.h/c
    activations.h/c
    optimizers.h/c

//
